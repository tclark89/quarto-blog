{
  "hash": "066515aac68674eaa43a85021a21a182",
  "result": {
    "markdown": "---\ntitle: \"Process Control Charts\"\ndate: \"2023-01-20\"\ncategories: [Process Control, R, ggplot2, engineering, manufacturing, tidyverse, code]\nimage: \"fig-x-bar-and-s-lithograph-WECO-1.png\"\nexecute: \n  warning: false\n---\n\n\n## Process Control Charts\n\nProcess Control Charts are great for analyzing any sort of process and identifying\n  shifts in the process and outliers. They are a fundamental part of the Six Sigma\n  program. \n  \nThere are R libraries for making these charts:\n\n- [qcc](https://cran.r-project.org/web/packages/qcc/index.html)\n- [qicharts2](https://cran.r-project.org/web/packages/qicharts2/vignettes/qicharts2.html)\n- [ggQC](https://cran.r-project.org/web/packages/ggQC/index.html)\n- [And many more...](https://www.google.com/search?q=process+control&domains=r-project.org&sitesearch=r-project.org)\n\n\nBut I prefer to do them from scratch for more customization.\n\nThere are 3 main types of SPC Charts:\n\n- X-Bar and S (Std. Dev, >10 samples per run/group)\n- X-Bar and R (run range, <= 10 per run/group)\n- X-Bar and M (change from run to run, when there is only 1 measure per group)\n\nA great resource for learning about Process Control Charts and other statistical\n  methods is the [NIST Handbook](https://www.itl.nist.gov/div898/handbook/index.htm),\n  which is referenced heavily for this post.\n  \nNot only does it provide information and formulas, but data too. I will be using\n the [LITHOGRA.DAT](https://www.itl.nist.gov/div898/handbook/datasets/LITHOGRA.DAT)\n data for the charts in this post.\n\n\n## X-bar and S Charts\n\n### Lithograph Data\n\nLet's look at an example of creating a set of X-bar and S charts.\n  We'll use the \n  [Lithograph](https://www.itl.nist.gov/div898/handbook/datasets/LITHOGRA.DAT) \n  dataset from the NIST handbook. This data measures the width of lines\n  etched in 5 locations on silicon wafers. \n  There are 3 wafers per cassette, and 30 cassettes for a grand \n  total of 450 measurements. \n  \nThere are many different approaches we could take to how to group this\n  data, but for now, lets consider the cassette as our \"run\" and examine\n  the distribution of line widths for each cassette, regardless of the\n  wafer number (1, 2, or 3) or the location of the line. \n  \n### Reading the data\n\nWe'll use `readr::read_fwf()` to read this data since it is in a \n  fixed-width format. We'll also drop the last column as it is not\n  necessary (see the text in the file header for more details).\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\n\n# Load the link, or download the .DAT file and substitute in the next step.\nfileLitho <- read_file(\"https://www.itl.nist.gov/div898/handbook/datasets/LITHOGRA.DAT\")\n\n# read\ndataLitho <- read_fwf(\n  fileLitho, \n  col_positions = fwf_widths(\n    c(7, 7, 7, 12, 7, 12),\n    col_names=c(\"CASSETTE\", \"WAFER\", \"SITE\", \n                \"LINEWIDT\", \"RUNSEQ\",\"LINEWIDT_2\")\n    ),\n  skip=25\n) |> \n  subset(select=-c(LINEWIDT_2))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(dataLitho)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 Ã— 5\n  CASSETTE WAFER  SITE LINEWIDT RUNSEQ\n     <dbl> <dbl> <dbl>    <dbl>  <dbl>\n1        1     1     1     3.20      1\n2        1     1     2     2.25      2\n3        1     1     3     2.07      3\n4        1     1     4     2.42      4\n5        1     1     5     2.39      5\n6        1     2     1     2.65      6\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(dataLitho)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    CASSETTE        WAFER        SITE      LINEWIDT          RUNSEQ     \n Min.   : 1.0   Min.   :1   Min.   :1   Min.   :0.7465   Min.   :  1.0  \n 1st Qu.: 8.0   1st Qu.:1   1st Qu.:2   1st Qu.:2.0505   1st Qu.:113.2  \n Median :15.5   Median :2   Median :3   Median :2.4533   Median :225.5  \n Mean   :15.5   Mean   :2   Mean   :3   Mean   :2.5323   Mean   :225.5  \n 3rd Qu.:23.0   3rd Qu.:3   3rd Qu.:4   3rd Qu.:2.9697   3rd Qu.:337.8  \n Max.   :30.0   Max.   :3   Max.   :5   Max.   :5.1687   Max.   :450.0  \n```\n:::\n:::\n\n\n### Explore the data\n\nThe first step of any data analysis, even when you have a clear goal\n  in mind, should be to explore the data and get a feel for what is\n  going on in there. Let's load `ggplot` (we'll need it later anyway)\n  and poke around.\n  \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n```\n:::\n\n\nLet's use box-and-whisker plots to check out the groups.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLitho |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT, group=CASSETTE)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![Linewidth by Cassette](index_files/figure-html/fig-lithograph-linewidth-cassette-1.png){#fig-lithograph-linewidth-cassette width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelCassette <- lm(LINEWIDT~CASSETTE, data=dataLitho)\nsummary(modelCassette)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = LINEWIDT ~ CASSETTE, data = dataLitho)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.76979 -0.44244 -0.03813  0.40676  2.26966 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.03801    0.06160  33.086   <2e-16 ***\nCASSETTE     0.03189    0.00347   9.191   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6371 on 448 degrees of freedom\nMultiple R-squared:  0.1586,\tAdjusted R-squared:  0.1568 \nF-statistic: 84.47 on 1 and 448 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nIt looks like there is some change in line width as the process goes on. \n  This is exactly the sort of thing that SPC charts look for. \n\n\n  \n### Initial SPC Calculations\n\nThe first step of creating the SPC charts is to find the mean, standard deviation,\n  and count of measurements in each group. This can be done in base R using\n  `aggregate()` and `transform()`, or with `dplyr` functions. Both methods are\n  shown below:\n  \n::: {.panel-tabset}\n\n#### base R\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoSumm <- aggregate(\n  # group LINEWIDT by CASSETTE\n  LINEWIDT~CASSETTE, \n  data=dataLitho,\n  # Calculate group mean, sd, and counts\n  FUN=function(x) c(mean=mean(x), \n                    sd=sd(x), \n                    count=length(x))\n)\n\n# We get a funky data.frame from aggregate with 1 normal column and 1 matrix column. \n# do.call will expand the matrix into separate columns. \n# It's a bit like nest and unnest in dplyr. \ndataLithoSumm <- do.call(data.frame, dataLithoSumm)\n\n\ndataLithoSumm <- transform(\n  dataLithoSumm,\n  \n  # Now we calculate the mean of means (x-bar-bar in the NIST handbook)\n  process.mean = mean(LINEWIDT.mean),\n\n  # and the mean of the standard deviations\n  process.sd = mean(LINEWIDT.sd)\n  )\n```\n:::\n\n\nWith some nesting and use of R's native pipe, this could all be run as one command,\n  but it starts to look messy, and code legibility is important in itself. \n\n#### dplyr\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\ndataLithoSumm <- dataLitho |> \n  \n  # first set the group\n  group_by(CASSETTE) |> \n  \n  # Mean, SD, and Count for each run\n  summarise(LINEWIDT.mean = mean(LINEWIDT),\n            LINEWIDT.sd = sd(LINEWIDT),\n            LINEWIDT.count = n()) |> \n  \n  # Ungroup the data for the next step\n  ungroup() |> \n  \n  # Overall process Mean and SD\n  mutate(\n    process.mean = mean(LINEWIDT.mean),\n    process.sd = mean(LINEWIDT.sd)\n  ) \n```\n:::\n\n\nOne of the great things about the tidyverse libraries is the ability to do many\n  separate steps in a string of easy to understand commands. \n\n:::\n\nThe only difference in the output is that base R produces a data.frame and dplyr\n  produces a \n  [tibble](https://tibble.tidyverse.org/), \n  but a \n  [tibble is really just a data.frame with some extensions](https://r4ds.had.co.nz/tibbles.html)\n  \nNow that we have some summary data, let's take a look at it:\n\n::: {.panel-tabset}\n\n#### Mean\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoSumm |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.mean)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(aes(yintercept = process.mean))\n```\n\n::: {.cell-output-display}\n![Mean Linewidth by Cassette and Overall](index_files/figure-html/fig-litho-mean-1.png){#fig-litho-mean width=672}\n:::\n:::\n\n\n\n#### Standard Deviation\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoSumm |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.sd)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(aes(yintercept = process.sd))\n```\n\n::: {.cell-output-display}\n![Std. Dev. of Linewidth by Cassette](index_files/figure-html/fig-litho-sd-1.png){#fig-litho-sd width=672}\n:::\n:::\n\n\n:::\n\nIt looks like we're off to a good start. We've now got the makings\nof a pair of X-bar and S charts, so now we need to add the next layer.\n\n### Control Limits\n\nFor standard Shewhart control charts use a 3-sigma range, or +/-\n  3 standard deviations from the mean (the namesake of 6-Sigma).\n  Technically, we do not know the true mean and standard \n  deviation of the process, only the measured estimates. We account\n  for this uncertainty with a weighting factor. \n  There is a full explanation of this in the \n  [NIST Engineering Statistics Handbook](https://www.itl.nist.gov/div898/handbook/pmc/section3/pmc32.htm).\n  \nTo calculate the Control Limits, we'll need the c$_{4}$ parameter:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# C4 Function\nc4 <- function(n) {\n  sqrt(2/(n-1)) * (factorial(n/2-1) / factorial((n-1)/2-1))\n}\n```\n:::\n\n\nThen we use this parameter to calculate the Upper and Lower Control \n  Limits:\n\n::: {.panel-tabset}\n#### Base R\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoSumm <- dataLithoSumm |> \n  transform(\n    \n    # X-bar chart UCL & LCL\n    xBar.UCL = process.mean + 3 * process.sd /\n      (c4(LINEWIDT.count)*sqrt(LINEWIDT.count)),\n    \n    xBar.LCL = process.mean - 3 * process.sd /\n      (c4(LINEWIDT.count)*sqrt(LINEWIDT.count)),\n    \n    # S chart UCL & LCL\n    s.UCL = process.sd + 3 * (process.sd/c4(LINEWIDT.count)) *\n      sqrt(1 - c4(LINEWIDT.count)^2),\n    \n    s.LCL = process.sd - 3 * (process.sd/c4(LINEWIDT.count)) *\n      sqrt(1 - c4(LINEWIDT.count)^2)\n    \n  )\n```\n:::\n\n\n#### dplyr\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoSumm <- dataLithoSumm |> \n  mutate(\n    \n    # X-bar chart UCL & LCL\n    xBar.UCL = process.mean + 3 * process.sd /\n      (c4(LINEWIDT.count)*sqrt(LINEWIDT.count)),\n    \n    xBar.LCL = process.mean - 3 * process.sd /\n      (c4(LINEWIDT.count)*sqrt(LINEWIDT.count)),\n    \n    # S chart UCL & LCL\n    s.UCL = process.sd + 3 * (process.sd/c4(LINEWIDT.count)) *\n      sqrt(1 - c4(LINEWIDT.count)^2),\n    \n    s.LCL = process.sd - 3 * (process.sd/c4(LINEWIDT.count)) *\n      sqrt(1 - c4(LINEWIDT.count)^2)\n    \n  )\n```\n:::\n\n\n\n:::\n\nThere's a lot of repetitive code up there, some of that could be condensed\n  if desired. Also, since every group in this dataset has the same number\n  of measurements, the upper and lower control limits for the X-bar\n  and S charts are the same for each group. Of course the mean of \n  means and mean of standard deviations, `process.mean` and `process.sd`,\n  are also the same for each group. These could be calculated as \n  standalone variables if desired. Just remember that you can only do that\n  if every row has the same sample size, otherwise you do have to calculate\n  the limits for each row. \n  \n### Plotting the data\n\nNow that we have the data, lets make these charts.\n  \n\n::: {.cell}\n\n```{.r .cell-code}\ng1 <- dataLithoSumm |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.mean)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(aes(yintercept=process.mean)) +\n  geom_line(aes(y=xBar.UCL)) +\n  geom_line(aes(y=xBar.LCL)) +\n  labs(y=\"Avg. Linewidth\", x=\"Cassette\") +\n  theme_bw()\n\n\ng2 <- dataLithoSumm |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.sd)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(aes(yintercept=process.sd)) +\n  geom_line(aes(y=s.UCL)) +\n  geom_line(aes(y=s.LCL)) +\n  labs(y=\"StdDev. Linewidth\", x=\"Cassette\") +\n  theme_bw()\n\n\ngridExtra::grid.arrange(g1, g2)\n```\n\n::: {.cell-output-display}\n![Lithograph X-Bar and S Charts](index_files/figure-html/fig-x-bar-and-s-lithograph-1.png){#fig-x-bar-and-s-lithograph width=672}\n:::\n:::\n\n\nGenerally the X-Bar and S charts are shown together with the x-axes aligned. \n\nIf desired, the [Western Electric Company Rules](https://www.itl.nist.gov/div898/handbook/pmc/section3/pmc32.htm)\n  can be applied to this chart. \n  I like to mainly focus on shifting averages and extreme outliers:\n  \n\n#### dplyr\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoSumm <- dataLithoSumm |> \n  mutate(\n    # Is the point beyond +/- 3 sigma?\n    xBar.beyond = ifelse((LINEWIDT.mean > xBar.UCL | LINEWIDT.mean < xBar.LCL), 1, 0),\n    # Assign each point -1, 0, or 1 if it is below, at, or above the mean\n    xBar.position = ifelse(LINEWIDT.mean > process.mean, 1, \n                       ifelse(LINEWIDT.mean < process.mean, -1, 0)),\n    # Then cumulatively sum these scores\n    xBar.csum = cumsum((xBar.position)),\n    # Use dplyr::lag to see if there are 7 or more consecutive points above or\n    # below the overall mean\n    xBar.lag = xBar.csum - lag(xBar.csum, 7, default = 0),\n    xBar.violatingRun = if_else(abs(xBar.lag)>=7, 1, 0),\n    xBar.color=if_else(xBar.beyond == 1, \"Beyond Limits\", \n                  if_else(xBar.violatingRun==1, \"Violating Run\", \"Normal\")),\n    \n    \n    \n    # Repeat for the standard deviation\n    s.beyond = ifelse((LINEWIDT.sd > s.UCL | LINEWIDT.sd < s.LCL), 1, 0),\n    s.position = ifelse(LINEWIDT.sd > process.sd, 1, \n                       ifelse(LINEWIDT.sd < process.sd, -1, 0)),\n    s.csum = cumsum((s.position)),\n\n    s.lag = s.csum - lag(s.csum, 7, default = 0),\n    s.violatingRun = if_else(abs(s.lag)>=7, 1, 0),\n    s.color=if_else(s.beyond == 1, \"Beyond Limits\", \n                  if_else(s.violatingRun==1, \"Violating Run\", \"Normal\"))\n  )\n```\n:::\n\nI don't know of a good way to emulate `dplyr::lag()` without some ugly code to \n  shift row indexes around and then `merge(df1, df2, all.x=TRUE)`. \n\n  \n\n::: {.cell}\n\n```{.r .cell-code}\ncolorsKey <- c(\"Beyond Limits\"=\"red\", \"Violating Run\"=\"orange\", \"Normal\"=\"black\")\n\ng1 <- dataLithoSumm |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.mean)) +\n  geom_point(aes(color=xBar.color)) +\n  geom_line() +\n  geom_hline(aes(yintercept=process.mean)) +\n  geom_line(aes(y=xBar.UCL)) +\n  geom_line(aes(y=xBar.LCL)) +\n  labs(y=\"Avg. Linewidth\", x=\"Cassette\") +\n  scale_color_manual(values = colorsKey) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\ng2 <- dataLithoSumm |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.sd)) +\n  geom_point(aes(color=s.color)) +\n  geom_line() +\n  geom_hline(aes(yintercept=process.sd)) +\n  geom_line(aes(y=s.UCL)) +\n  geom_line(aes(y=s.LCL)) +\n  labs(y=\"StdDev. Linewidth\", x=\"Cassette\") +\n  scale_color_manual(values = colorsKey) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\ngridExtra::grid.arrange(g1, g2)\n```\n\n::: {.cell-output-display}\n![Lithograph X-Bar and S Charts With WECO Rules](index_files/figure-html/fig-x-bar-and-s-lithograph-WECO-1.png){#fig-x-bar-and-s-lithograph-WECO width=672}\n:::\n:::\n\n\n\n## X-bar and R Charts\n\nX-Bar and R charts are pretty similar to X-Bar and S charts, the just use Range\n  instead of Standard Deviation. These charts are mostly used when groups are \n  smaller than n=10. The Lithograph data has 15 measurements per cassette, but\n  it will be fine to use as an example.\n  \nWe'll drop some of the data from the Lithograph dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoSubset <- dataLitho |> \n  filter(WAFER < 3)\n```\n:::\n\n  \nOr use `subset()` for base R.\n  \n### SPC Calculations\n\nThe calculations for the mean will be the same, but we'll calculate Range instead\n  of Standard Deviation.\n\n::: {.panel-tabset}\n\n#### base R\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoRange <- aggregate(\n  # group LINEWIDT by CASSETTE\n  LINEWIDT~CASSETTE, \n  data=dataLithoSubset,\n  # Calculate group mean, sd, and counts\n  FUN=function(x) c(mean=mean(x), \n                    r=range(x)[2] - range(x)[1], \n                    count=length(x))\n)\n\n# We get a funky data.frame from aggregate with 1 normal column and 1 matrix column. \n# do.call will expand the matrix into separate columns. \n# It's a bit like nest and unnest in dplyr. \ndataLithoRange <- do.call(data.frame, dataLithoRange)\n\n\ndataLithoRange <- transform(\n  dataLithoRange,\n  \n  # Now we calculate the mean of means (x-bar-bar in the NIST handbook)\n  process.mean = mean(LINEWIDT.mean),\n\n  # and the mean of the standard deviations\n  process.range = mean(LINEWIDT.r)\n  )\n```\n:::\n\n\n#### dplyr\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoRange <- dataLithoSubset |> \n  \n  # first set the group\n  group_by(CASSETTE) |> \n  \n  # Mean, SD, and Count for each run\n  summarise(LINEWIDT.mean = mean(LINEWIDT),\n            LINEWIDT.r = range(LINEWIDT)[2] - range(LINEWIDT)[1],\n            LINEWIDT.count = n()) |> \n  \n  # Ungroup the data for the next step\n  ungroup() |> \n  \n  # Overall process Mean and SD\n  mutate(\n    process.mean = mean(LINEWIDT.mean),\n    process.range = mean(LINEWIDT.r)\n  ) \n```\n:::\n\n\n:::\n\n\n### Control Limits\n\nFor X-Bar and R charts, A$_{2}$ is generally used instead of c$_{4}$ for the X-Bar\n  unbiased estimator, and D$_{3}$ and D$_{4}$ are used for the Range:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntablexBarR <- data.frame(\n  n=c(2:10),\n  a2 = c(1.880, 1.023, 0.729, 0.577, 0.483, 0.419, 0.373, 0.337, 0.308),\n  d3 = c(0, 0, 0, 0, 0, 0.076, 0.136, 0.184, 0.223),\n  d4 = c(3.267, 2.575, 2.282, 2.115, 2.004, 1.924, 1.864, 1.816, 1.777)\n)\n```\n:::\n\n\nThen we use this parameter to calculate the Upper and Lower Control \n  Limits:\n\n::: {.panel-tabset}\n\n#### Base R\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoRange <- \n  merge(dataLithoRange, tablexBarR, \n        by.x = \"LINEWIDT.count\", by.y = \"n\") |> \n  transform(\n    \n    # X-bar chart UCL & LCL\n    xBar.UCL = process.mean + a2 * process.range,\n    xBar.LCL = process.mean - a2 * process.range,\n    \n    # S chart UCL & LCL\n    r.UCL = process.range * d4,\n    r.LCL = process.range * d3\n    \n  )\n```\n:::\n\n\n#### dplyr\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoRange <- dataLithoRange |> \n  left_join(tablexBarR, by=c(\"LINEWIDT.count\"=\"n\")) |> \n  mutate(\n    \n    # X-bar chart UCL & LCL\n    xBar.UCL = process.mean + a2 * process.range,\n    xBar.LCL = process.mean - a2 * process.range,\n    \n    # S chart UCL & LCL\n    r.UCL = process.range * d4,\n    r.LCL = process.range * d3\n    \n  )\n```\n:::\n\n\n:::\n\n### WECO Rules\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoRange <- dataLithoRange |> \nmutate(\n    \n    xBar.beyond = ifelse((LINEWIDT.mean > xBar.UCL | LINEWIDT.mean < xBar.LCL), 1, 0),\n    \n    xBar.position = ifelse(LINEWIDT.mean > process.mean, 1, \n                       ifelse(LINEWIDT.mean < process.mean, -1, 0)),\n    \n    xBar.csum = cumsum((xBar.position)),\n    \n    xBar.lag = xBar.csum - lag(xBar.csum, 7, default = 0),\n    xBar.violatingRun = if_else(abs(xBar.lag)>=7, 1, 0),\n    xBar.color=if_else(xBar.beyond == 1, \"Beyond Limits\", \n                  if_else(xBar.violatingRun==1, \"Violating Run\", \"Normal\")),\n    \n    \n    \n    \n    r.beyond = ifelse((LINEWIDT.r > r.UCL | LINEWIDT.r < r.LCL), 1, 0),\n    r.position = ifelse(LINEWIDT.r > process.range, 1, \n                       ifelse(LINEWIDT.r < process.range, -1, 0)),\n    r.csum = cumsum((r.position)),\n\n    r.lag = r.csum - lag(r.csum, 7, default = 0),\n    r.violatingRun = if_else(abs(r.lag)>=7, 1, 0),\n    r.color=if_else(r.beyond == 1, \"Beyond Limits\", \n                  if_else(r.violatingRun==1, \"Violating Run\", \"Normal\"))\n  )\n```\n:::\n\n\n### X-Bar and R Chart\n\n::: {.cell}\n\n```{.r .cell-code}\ncolorsKey <- c(\"Beyond Limits\"=\"red\", \"Violating Run\"=\"orange\", \"Normal\"=\"black\")\n\ng1 <- dataLithoRange |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.mean)) +\n  geom_point(aes(color=xBar.color)) +\n  geom_line() +\n  geom_hline(aes(yintercept=process.mean)) +\n  geom_line(aes(y=xBar.UCL)) +\n  geom_line(aes(y=xBar.LCL)) +\n  labs(y=\"Avg. Linewidth\", x=\"Cassette\") +\n  scale_color_manual(values = colorsKey) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\ng2 <- dataLithoRange |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.r)) +\n  geom_point(aes(color=r.color)) +\n  geom_line() +\n  geom_hline(aes(yintercept=process.range)) +\n  geom_line(aes(y=r.UCL)) +\n  geom_line(aes(y=r.LCL)) +\n  labs(y=\"Range of Linewidths\", x=\"Cassette\") +\n  scale_color_manual(values = colorsKey) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\ngridExtra::grid.arrange(g1, g2)\n```\n\n::: {.cell-output-display}\n![Lithograph X-Bar and R Charts With WECO Rules](index_files/figure-html/fig-x-bar-and-r-lithograph-WECO-1.png){#fig-x-bar-and-r-lithograph-WECO width=672}\n:::\n:::\n\n\n## I and MR Charts\n\nWhen the sample size is n=1, use the I and MR, or Individuals and Moving Range, chart. \n  The Moving Range is the absolute difference between consecutive measurements. \n\n### Prepare the data\n\n::: {.cell}\n\n```{.r .cell-code}\ndataLithoMR <- dataLitho |>\n  # Filter down to one measurement per cassette\n  filter(WAFER == 1, SITE == 1) |> \n  mutate(\n    # Find the Moving Range\n    LINEWIDT.mr = abs(LINEWIDT - lag(LINEWIDT, 1)),\n    \n    process.mean = mean(LINEWIDT),\n    process.mr = mean(LINEWIDT.mr, na.rm=T),\n    \n    # 1.128 is the value for d2=1\n    xBar.UCL = process.mean + 3 * process.mr / 1.128,\n    xBar.LCL = process.mean - 3 * process.mr / 1.128,\n    \n    # 3.267 is D4 when n=2, 0 is D3 when n=2\n    mr.UCL = process.mr*3.267,\n    mr.LCL = process.mr*0,\n    \n    # Critical Limits for Individuals\n    xBar.beyond = if_else((LINEWIDT > xBar.UCL | LINEWIDT < xBar.LCL), 1, 0),\n    \n    xBar.position = ifelse(LINEWIDT > process.mean, 1, \n                       if_else(LINEWIDT < process.mean, -1, 0)),\n    \n    xBar.csum = cumsum((xBar.position)),\n    \n    xBar.lag = xBar.csum - lag(xBar.csum, 7, default = 0),\n    xBar.violatingRun = if_else(abs(xBar.lag)>=7, 1, 0),\n    xBar.color=if_else(xBar.beyond == 1, \"Beyond Limits\", \n                  if_else(xBar.violatingRun==1, \"Violating Run\", \"Normal\")),\n    \n    \n    \n    # Critical Limits for Moving Ranges\n    mr.beyond = if_else((LINEWIDT.mr > mr.UCL | LINEWIDT.mr < mr.LCL), 1, 0),\n    mr.beyond = if_else(is.na(mr.beyond), 0, mr.beyond),\n    \n    mr.position = if_else(LINEWIDT.mr > process.mr, 1, \n                       if_else(LINEWIDT.mr < process.mr, -1, 0)),\n    mr.position = if_else(is.na(mr.position), 0, mr.position),\n    mr.csum = cumsum(mr.position),\n\n    mr.lag = mr.csum - lag(mr.csum, 7, default = 0),\n    mr.violatingRun = if_else(abs(mr.lag)>=7, 1, 0),\n    mr.color=if_else(mr.beyond == 1, \"Beyond Limits\", \n                  if_else(mr.violatingRun==1, \"Violating Run\", \"Normal\"))\n    \n  ) \n```\n:::\n\n\n### I and MR Charts\n\n::: {.cell}\n\n```{.r .cell-code}\ncolorsKey <- c(\"Beyond Limits\"=\"red\", \"Violating Run\"=\"orange\", \"Normal\"=\"black\")\n\ng1 <- dataLithoMR |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT)) +\n  geom_point(aes(color=xBar.color)) +\n  geom_line() +\n  geom_hline(aes(yintercept=process.mean)) +\n  geom_line(aes(y=xBar.UCL)) +\n  geom_line(aes(y=xBar.LCL)) +\n  labs(y=\"Linewidth\", x=\"Cassette\") +\n  scale_color_manual(values = colorsKey) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\ng2 <- dataLithoMR |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.mr)) +\n  geom_point(aes(color=mr.color)) +\n  geom_line() +\n  geom_hline(aes(yintercept=process.mr)) +\n  geom_line(aes(y=mr.UCL)) +\n  geom_line(aes(y=mr.LCL)) +\n  labs(y=\"Moving Range of Linewidths\", x=\"Cassette\") +\n  scale_color_manual(values = colorsKey) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\ngridExtra::grid.arrange(g1, g2)\n```\n\n::: {.cell-output-display}\n![Lithograph I and MR Charts With WECO Rules](index_files/figure-html/fig-i-and-mr-lithograph-WECO-1.png){#fig-i-and-mr-lithograph-WECO width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}