[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nProcess Control Charts\n\n\n17 min\n\n\n\nProcess Control\n\n\nR\n\n\nggplot2\n\n\nengineering\n\n\nmanufacturing\n\n\ntidyverse\n\n\ncode\n\n\n\n\n\n\n\nJan 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Approach to Tables in Quarto Documents\n\n\n10 min\n\n\n\nquarto\n\n\nrmarkdown\n\n\nknitr\n\n\nkableExtra\n\n\nflextable\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing R and RStudio with OneDrive/SharePoint\n\n\n2 min\n\n\n\nnotes\n\n\nR\n\n\nRStudio\n\n\nMicrosoft\n\n\nOneDrive\n\n\nSharePoint\n\n\n\n\n\n\n\nSep 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDplyr’s case_when\n\n\n3 min\n\n\n\ntidyverse\n\n\ndplyr\n\n\nnotes\n\n\ncode\n\n\nR\n\n\n\n\n\n\n\nSep 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Home RStudio Setup\n\n\n6 min\n\n\n\nDocker\n\n\nRocker Project\n\n\nR\n\n\nRStudio\n\n\nLinux\n\n\n\n\n\n\n\nSep 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower Query Tip - Opening Multiple Files\n\n\n4 min\n\n\n\nMicrosoft\n\n\nPowerBI\n\n\nM Code\n\n\nExcel\n\n\ncode\n\n\nnotes\n\n\n\n\n\n\n\nSep 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome Notes About Flatpak\n\n\n2 min\n\n\n\nflatpak\n\n\nlinux\n\n\nnotes\n\n\ndebian\n\n\narchlinux\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy favorite R packages\n\n\n1 min\n\n\n\nR\n\n\nRstudio\n\n\ntidyverse\n\n\n\n\n\n\n\nSep 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n0 min\n\n\n\nnews\n\n\n\n\n\n\n\nSep 3, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Process Control Charts\n\n\n\n\n\n\n\n\n\nJan 20, 2023\n\n\n\n\n\n\n\n\nMy Approach to Tables in Quarto Documents\n\n\n\n\n\n\n\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\nUsing R and RStudio with OneDrive/SharePoint\n\n\n\n\n\n\n\n\n\nSep 20, 2022\n\n\n\n\n\n\n\n\nDplyr’s case_when\n\n\n\n\n\n\n\n\n\nSep 13, 2022\n\n\n\n\n\n\n\n\nMy Home RStudio Setup\n\n\n\n\n\n\n\n\n\nSep 7, 2022\n\n\n\n\n\n\n\n\nPower Query Tip - Opening Multiple Files\n\n\n\n\n\n\n\n\n\nSep 5, 2022\n\n\n\n\n\n\n\n\nSome Notes About Flatpak\n\n\n\n\n\n\n\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\nMy favorite R packages\n\n\n\n\n\n\n\n\n\nSep 3, 2022\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\nSep 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-09-03-my-favorite-packages/index.html",
    "href": "posts/2022-09-03-my-favorite-packages/index.html",
    "title": "My favorite R packages",
    "section": "",
    "text": "When I first started out doing data analysis professionally, I tried not to stray from base R. Why? I’m not sure. After all, I had to install something to read Excel files… At any rate, at some point I decided to have IT install RStudio’s IDE on my laptop and to try out this “tidyverse” thing.\nMind Blown\nJust the %>% operator alone radically changed my workflow and caused me to refactor all of my code (Hell, even R is integrating a pipe function now: |> in R 4.1+). Then learning how to use all of the data manipulation tools in dplyr (mutate, select, separate, oh how I could go on) made working with data exciting and intelligible instead of some chore full of arcane commands and confusing code.\nRMarkdown and now Quarto have given me many new avenues for reporting data, results, and analysis, even though I am still heavily tied to having to email an Excel file for some of the old-school folks (I guess if it ain’t broke…). Well, that’s why I learned some VBA too."
  },
  {
    "objectID": "posts/2022-09-03-welcome/index.html",
    "href": "posts/2022-09-03-welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my blog. Currently this blog is really just a collection of ideas and code that I find useful. Maybe you will too.\nThe github repository for the entire site can be found here."
  },
  {
    "objectID": "posts/2022-09-04-flatpak/index.html",
    "href": "posts/2022-09-04-flatpak/index.html",
    "title": "Some Notes About Flatpak",
    "section": "",
    "text": "After a few years of running ArchLinux on my NAS PC, I recently (a couple weeks ago?) decided to switch to a solid foundation of Debian Stable (Bullseye 11.4 at the time of this post), with most of my userland apps installed via Flatpak.\n\n\n\nFlatpak\n\n\nOne is not better than the other, but I was tired of having to regularly check for updates, decide if they looked important, check the homepage for any breakage news, etc. It’s not so bad for a desktop that is used for tinkering and playing, but for a fileserver running programs that I’d rather not restart all the time (hold on, let me pause this show on Plex and make sure my backup NAS isn’t syncing)…\nSo far so good. The overall system is smaller (due to Debian breaking up packages while Arch installs the kitchen sink at times), and it makes my root backup tarball smaller since I can leave out the flatpak directory and focus on the actual system."
  },
  {
    "objectID": "posts/2022-09-04-flatpak/index.html#flatpak-notes",
    "href": "posts/2022-09-04-flatpak/index.html#flatpak-notes",
    "title": "Some Notes About Flatpak",
    "section": "Flatpak Notes",
    "text": "Flatpak Notes\nAnyway, the main notes that I meant to put here are:\n\nflatpak remote-add flathub flathub-url is not the same as sudo flatpak remote-add flathub flathub-url\n*For a single-user system there’s not that much real difference in terms of the experience, but installing flathub without sudo only installs the repo for YOU and then any apps you install get saved in /home. You do you.\nFlatpak apps are installed at /var/lib/flatpak, so keep that as a separate BTRFS subvolume to exclude them from root backups.\nFor apps, MakeMKV in particular, the app’s personal root is /app/, not /.\nUser config files and data are mostly stored per-app in ~/.var/, but some apps have access to /home/user.\nAdd /var/lib/flatpak/exports/bin to you $PATH to more easily call the apps from the command line, in case error codes need to be looked at.\n\nOverall I’m pretty happy with the change. Most of the gui programs that I use are in flathub, so that’s pretty easy. I was worried about bloat and performance, but it’s really not that bad. Apps are sandboxed, yes, but they can share Flatpak libraries and runtimes so there isn’t so much redundancy. It’s really nice when installing a single Gnome app or Wine not to have to install an entire suite of Gnome apps and multi-lib stuff just for one program."
  },
  {
    "objectID": "posts/2022-09-05-powerbi-loop-files/index.html",
    "href": "posts/2022-09-05-powerbi-loop-files/index.html",
    "title": "Power Query Tip - Opening Multiple Files",
    "section": "",
    "text": "While I prefer to use R/RMarkdown/Quarto as much as possible for data wrangling and reporting, sometimes the right tool is something else. Many organizations are heavily tied in to Microsoft’s ecosystem and it is what lots of managers and executives are familiar with.\nIn this sort of environemnt, the suite of Power tools: PowerBI, Power Query, Power Pivot, etc. is a good choice. As a bonus, they play nicely with OneDrive and SharePoint, and PowerBI allows for automated data updating.\nIn my day-to-day I run into situations where I need to combine several Excel files that are split into chunks, say some sort of accounting information that is saved in separate files per year. Power Query’s GUI can help to get you started, but if you need to do any more complicated data wrangling you’ll need to dive into M code.\nA common template I use for the above scenario is something like this:\nlet\n  // Navigate to the SharePoint library (not folder) with the files.\n  Source = SharePoint.Files(\n    \"https://contoso.sharepoint.com/personal/tyler_contoso_com\",\n    [ApiVersion = 15]\n  ),\n  \n  // Filter the file list down to the correct folder\n  #\"Filtered Folders\" = Table.SelectRows(\n    Source, \n    each [Folder Path] = \"https://contoso.sharepoint.com/personal/tyler_contoso_com/Documents/Documents/Widget Exports/\"),\n    \n  // Filter down to just the right files.\n  // In this example, the files are named \"2022 Widget_Exports.xlsx\", \n  // \"2021 Widget_exports.xlsx\", etc.\n  #\"Filtered Files\" = Table.SelectRows(\n    #\"Filtered Folders\", \n    each Text.Contains([Name], \"Widget_Exports.xlsx\")\n    ),\n    \n  // Excel Opener Function\n  ExcelOpener = (folderPath, fileName) => \n  let\n    // This function nested in the main function processes all of\n    // the Excel files in the same way. It does not change the\n    // column types yet, that is saved for the end. I have had the \n    // column type information get tossed out in a following step,\n    // that's why it doesn't get defined here.\n    // The function does name the columns. \n    SelectFile = #\"Filtered Files\"{\n      [Name = fileName, \n      #\"Folder Path\" = folderPath]}[Content],\n      \n      #\"Imported Excel\" = Excel.Workbook(SelectFile, null, true),\n      \n      #\"Navigation\" = #\"Imported Excel\"{\n      [Item = \"Sheet\", Kind = \"Sheet\"]\n      }[Data],\n      \n      #\"Filtered rows\" = Table.SelectRows(\n        // If there are any merged cells, this can toss unnecessary rows\n        #\"Navigation\", each [Column2] <> null),\n        \n      #\"Promoted headers\" = Table.PromoteHeaders(\n      #\"Filtered rows\", [PromoteAllScalars = true]\n      )\n  in\n    #\"Promoted headers\",\n  \n  // To use the function, make a new column and get its values from the\n  // function. It will be a nested data column.\n  #\"Added Custom\" = Table.AddColumn(\n    #\"Filtered Files\", \n    \"Custom\", \n    each ExcelOpener([Folder Path], [Name])\n    ),\n  \n  // We can remove everything but the new column  \n  #\"Removed columns\" = Table.RemoveColumns(\n    #\"Added Custom\", \n    {\"Content\", \"Name\", \"Extension\", \"Date accessed\", \n      \"Date modified\", \"Date created\", \"Attributes\", \n      \"Folder Path\"}\n    ),\n  \n  // Expand the column and select all of the named columns  \n  #\"Expanded Custom\" = Table.ExpandTableColumn(\n    #\"Removed columns\", \n    \"Custom\", \n    {\"Acct No\", \"Cust No\", \"PO\", \"Transaction Date\", \"Widget Type\", \n      \"Widget Cost Per Unit\", \"Total Units\", \"Total Cost\", \n     \"Shipped\", \"Shipped Date\"}\n    ),\n  \n  // Now we will set the column types\n  #\"Changed column type\" = Table.TransformColumnTypes(\n    #\"Expanded Custom\", \n    {\n      {\"Acct No\", type text}, \n      {\"Cust No\", type text}, \n      {\"PO\", type text}, \n      {\"Transaction Date\", type date}, \n      {\"Widget Type\", type text}, \n      {\"Widget Cost Per Unit\", type number}, \n      {\"Total Units\", Int64.Type}, \n      {\"Total Cost\", type number}, \n      {\"Shipped\", logical}, \n      {\"Shipped Date\", type date}\n      }\n    )\nin\n  #\"Changed column type\"\nUsing code similar to this will combine the directory full of files into one big Power dataset that you can now modify as needed for the analysis or report that you are using."
  },
  {
    "objectID": "posts/2022-09-07-my-rstudio-setup/index.html",
    "href": "posts/2022-09-07-my-rstudio-setup/index.html",
    "title": "My Home RStudio Setup",
    "section": "",
    "text": "When I am working on, well, work-related work, I use my company-issued laptop with Windows, Microsoft Office tools, and an installation of RStudio Desktop.\nRecently I have been wanting to setup a personal installation of RStudio on my home file server as well. The main reasons being that RStudio makes for a great general-purpose IDE, and I’ve been wanting to start working with Quarto on some things, so right now seemed like a great time to figure it out."
  },
  {
    "objectID": "posts/2022-09-07-my-rstudio-setup/index.html#backstory",
    "href": "posts/2022-09-07-my-rstudio-setup/index.html#backstory",
    "title": "My Home RStudio Setup",
    "section": "Backstory",
    "text": "Backstory\nI wrote in a previous post that I am working on making my home file server more stable, which means moving more of the userland packages to containers. Most of the programs I use on that PC (Firefox, VLC, Gimp, etc.) all have flatpaks that work quite well for me.\nHowever, R and RStudio are not in that category. On Debian, both require going outside of the official Debian repositories: R has a repository for up-to-date versions, while RStudio does not (although they do provide binary downloads). Add to that the system dependencies of some packages and now there are random libraries installed all over, with no dependency chain in apt. There is the option to use the Debian packages from the CRAN repository, but those are not complete and have given me issues in the past, as far as permissions, updates, etc.\nIt seemed to me that R would need it’s own environment. How best to do that? I turned to Docker for this particular case.\n\n\n\nDocker"
  },
  {
    "objectID": "posts/2022-09-07-my-rstudio-setup/index.html#docker-setup",
    "href": "posts/2022-09-07-my-rstudio-setup/index.html#docker-setup",
    "title": "My Home RStudio Setup",
    "section": "Docker Setup",
    "text": "Docker Setup\n\nBase Image\nLuckily, smarter people than myself have already thought about this and started the Rocker Project. They have many different containers all built in layers on each other so it’s easy to find a good starting point for running R/shiny apps, or building a general dev environment like I was looking for.\n\n\nAdding on Layers\nAt the time of this post I chose the rocker/tidyverse image to build my RStudio environment, but this could change in the future (see my repo for the latest).1 Having Rstudio server and the tidyverse packages gets me 90% of the way to where I want to be, but there are other packages that I use quite a bit. To get those packages I would either need to\n\nOption 1:\n\nSpin up an image,\nDownload the packages in the image\nSave that modified environment as a new image via docker commit\n\nOption 2:\n\nCreate a Dockerfile and let docker build handle it\n\n\nI am a big fan of keeping things simple, reproducible, and mostly in line with the intended workflow (sometimes rules are made to be broken, but I am not familiar enough with docker to be getting my hands that dirty just yet), so I went with Option 2.\n\n\nConfiguration\nAt the time of this post, this is my Dockerfile:\nFROM rocker/tidyverse:4.2.1\nCOPY packages.R /home/rstudio/packages.R\nRUN R -q -e \"source('/home/rstudio/packages.R')\" \\\n    && rm -rf /tmp/* \\\n    && strip /usr/local/lib/R/site-library/*/libs/*.so\nI start with the rocker/tidyverse:4.2.1 image, then copy an R script into the rstudio home directory:\ninstall.packages(\n         c(\n           \"markdown\",\n           \"gt\", \n           \"DT\", \n           \"kableExtra\", \n           \"flextable\", \n           \"huxtable\", \n           \"reactable\", \n           \"formattable\", \n           \"pixiedust\", \n           \"agricolae\", \n           \"car\"\n         )\n)\nAfter the script is run, I execute 2 more commands to clean up the image (as advised by the Rocker team). This image is available on Docker Hub as tclark89/tidyverse-extra\nOne day I may dig more deeply into setting up my own very custom image by building something more from scratch, but for now the rocker/tidyverse image works as great jumping-off point.\n\n\nCompose\nNow that the image has been created it needs to be spun up, and it needs to be run with certain parameters. The best way to do that is with a docker-compose.yml file. Currently mine looks like this:\nservices:\n  rstudio:\n    image: tclark89/tidyverse-extra:4.2.1\n    ports: \n      - \"8787:8787\"\n    environment:\n      PASSWORD: rstudio\n      ROOT: true\n    volumes:\n      - ~/.config/rstudio:/home/rstudio/.config/rstudio\n      - ~/.local/share/rstudio:/home/rstudio/.local/share/rstudio\n      - ~/code/R:/home/rstudio/workspace\nThe docker-compose.yml file starts the rstudio service:\n\nUses my custom docker image as the base\nMaps the container ports\nSets the rstudio user’s password and gives it sudo via environment variables\nMaps some directories to be shared between host and container. This lets settings and files persist in my /home directory between sessions.\n\nThe rocker team provides an example compose file as well.\n\n\nRunning the Container\n All it takes to run the docker container now is to cd to the directory with the docker-compose.yml file and issue the command: docker compose up. However, I would like for this to happen automatically so I can just login and go. There may be a different way to do this, but I went with a systemd service file.\nAfter some digging through the Arch Wiki and various SO posts I settled on the following:\n[Unit]\nDescription=%i service with docker compose\nPartOf=docker.service\nAfter=docker.service\n\n[Service]\nType=oneshot\nRemainAfterExit=true\nWorkingDirectory=/home/%u/docker/compose/%i\nExecStart=/usr/bin/docker compose up -d --remove-orphans\nExecStop=/usr/bin/docker-compose down\n\n[Install]\nWantedBy=multi-user.target\nThis service file is a user @.service, so the file is named docker-compose@.service and called via:\n\nsystemctl start --user docker-compose@tidyverse-extra.service\n\nor for a persistent setup:\n\nsystemctl enable --now --user docker-compose@tidyverse-extra.service\n\nBecause docker compose looks for a docker-compose.yml file in the working directory, I needed to specify one in this file and make sure that directory existed in my /home. The %u makes this file user-agnostic and the %i and @ make it usable for any other docker service I may decide to use later. I just need to make sure that I create a directory in ~/docker/compose/ and put the docker-compose.yml file there.\nAs a final note, this will only work if your user is part of the docker group, otherwise docker needs root privileges. This file could just as easily be made to work as root, but the docker-compose.yml file would need to have absolute paths instead of user-relative ones.\n\nUpdate: 11/7/2022\nDocker doesn’t recommend making images persistent with systemd files. There is a built-in method for starting containers at boot:\n\nStart the container with docker run, docker compose up, etc.\nRun the command: docker update --restart unless-stopped $container_name\n\nMuch simpler!"
  },
  {
    "objectID": "posts/2022-09-07-my-rstudio-setup/index.html#using-rstudio",
    "href": "posts/2022-09-07-my-rstudio-setup/index.html#using-rstudio",
    "title": "My Home RStudio Setup",
    "section": "Using RStudio",
    "text": "Using RStudio\nNow that all of the pieces are in place, all I have to do to access RStudio at home is to open a browser, access my file server at port 8787, and log in to RStudio Server. Then I can just git pull and get back to work on this website or whatever project I am focused on.\nFor now this all works wonderfully. I don’t have to muck around with dependencies on my server, and saving the docker files to GitHub and the image to Docker Hub means that it should be reproducible in case I can’t access the server. For instance, if I am away from home but want to work on a project I can spin up the docker image directly on my Chromebook (via the built-in Linux VM) and fire away. In fact, you can even run this image in a browser via Play with Docker, though it eats up quite a bit of RAM to do so."
  },
  {
    "objectID": "posts/2022-09-13-case_when/index.html",
    "href": "posts/2022-09-13-case_when/index.html",
    "title": "Dplyr’s case_when",
    "section": "",
    "text": "This is a very small post that’s really more of a note-to-self. I wind up using nested if_else statements a lot, but dplyr contains a nifty function that really eliminates the need for them and in turn cleans up the code. The case_when function operates very similarly to CASE WHEN THEN ELSE in SQL:\nSQL\nSELECT \n  CASE \n    WHEN x = 1 THEN 'a'\n    WHEN x = 2 THEN 'b'\n    WHEN x = 3 THEN 'c'\n    ELSE 'd'\n  END  \ndplyr\ncase_when(\n  x == 1 ~ \"a\",\n  x == 2 ~ \"b\",\n  x == 3 ~ \"c\",\n  TRUE ~ \"d\"\n)\nLike in SQL, dplyr::case_when() works its way down the list of conditionals in the order that they appear. It’s good practice to include the TRUE ~ result statement at the end. Sometimes you may want an “other” result, other times you may just want to make sure that you’ve caught all the rows.\nFor a more detailed example:\n\n# Let's load some libraries:\nlibrary(tibble)\nlibrary(dplyr)\n\n\n# I'll use mtcars, and we'll look at the car model\nmtcarsMod <- mtcars |> \n  rownames_to_column(var=\"model\")\nhead(mtcarsMod)\n\n              model  mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1         Mazda RX4 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n2     Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n3        Datsun 710 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n4    Hornet 4 Drive 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n5 Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n6           Valiant 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nLet’s say that we want to add the manufacturer as a separate column. There are a few ways to do it:\n\nNested if_else\nThe first way that always enters my head is to do nested if_else statements. It’s not terrible when there are only a few options, but they get ugly fast:\n\nmtcarsMod |> \n  mutate(make = if_else(grepl(\"Mazda\", model), \"Mazda\", \n                        if_else(grepl(\"Hornet\", model), \"Hornet\",\n                                if_else(grepl(\"Merc\", model), \"Mercury\",\n                                        if_else(grepl(\"Datsun\", model), \"Datsun\", \n                                                # etc.\n                                                \"Other\"))))) |> \n  select(make, names(mtcarsMod)) |>\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\n\nMultiple mutates\nOne way to make things cleaner is to do multiple mutates. Create the new column with your “catch-all” value, then use conditional statements that leave the new column unchanged when the condition is FALSE:\n\nmtcarsMod |> \n  mutate(\n    make = \"Other\",\n    make = if_else(grepl(\"Mazda\", model), \"Mazda\", make),\n    make = if_else(grepl(\"Hornet\", model), \"Hornet\", make),\n    make = if_else(grepl(\"Merc\", model), \"Mercury\", make),\n    make = if_else(grepl(\"Datsun\", model), \"Datsun\", make)\n    # etc.\n  ) |> \n  select(make, names(mtcarsMod)) |>\n  rmarkdown::paged_table()\n\n\n\n  \n\n\n\n\n\ncase_when\nBut the cleanest way (IMHO) is to use case_when. I don’t know why I always forget about it, but maybe the act of making this post will permanently etch it into my brain.\n\nmtcarsMod |> \n  mutate(make = case_when(\n    grepl(\"Mazda\", model) ~ \"Mazda\",\n    grepl(\"Hornet\", model) ~ \"Hornet\",\n    grepl(\"Merc\", model) ~ \"Mercury\",\n    grepl(\"Datsun\", model) ~ \"Datsun\",\n    # etc.\n    TRUE ~ \"Other\"\n  )) |> \n   select(make, names(mtcarsMod)) |>\n  rmarkdown::paged_table()"
  },
  {
    "objectID": "posts/2022-09-20-onedrive-and-r/index.html",
    "href": "posts/2022-09-20-onedrive-and-r/index.html",
    "title": "Using R and RStudio with OneDrive/SharePoint",
    "section": "",
    "text": "Love it or hate it, but Microsoft is fairly ubiquitous in any business environment. That could be reliance on Excel spreadsheets and Word documents up to full-blown Azure environments, PowerBI reports, etc.\nI have learned that there are some times when R and OneDrive butt heads, so here are some tips for making them get along:"
  },
  {
    "objectID": "posts/2022-09-20-onedrive-and-r/index.html#tip-1-dont-install-r-to-a-onedrive-folder",
    "href": "posts/2022-09-20-onedrive-and-r/index.html#tip-1-dont-install-r-to-a-onedrive-folder",
    "title": "Using R and RStudio with OneDrive/SharePoint",
    "section": "Tip 1: Don’t install R to a OneDrive folder",
    "text": "Tip 1: Don’t install R to a OneDrive folder\nNewer versions of R don’t suggest this anymore, but be sure not to install R into a OneDrive synced folder like Documents. It sounds like a good idea for portability, but updates will throw a great big wrench into things in a hurry, trust me. This also applies to related software like quarto, rtools, python/conda, really anything that can be installed locally as a normal user."
  },
  {
    "objectID": "posts/2022-09-20-onedrive-and-r/index.html#tip-2-no-r-projects-with-some-caveats",
    "href": "posts/2022-09-20-onedrive-and-r/index.html#tip-2-no-r-projects-with-some-caveats",
    "title": "Using R and RStudio with OneDrive/SharePoint",
    "section": "Tip 2: No R Projects (with some caveats)",
    "text": "Tip 2: No R Projects (with some caveats)\nUnfortunately, this will also throw a wrench into things. R Studio makes a lot tiny folders with very deep folder trees and you can very quickly run into Window’s character limits. This is a major issue for RMarkdown/Quarto documents, and will break the ability to view code output without rendering. And then when you do render, all of those tiny deeply-nested files get changed and OneDrive loses it’s mind trying to track the changes. In fact, even when not using R Projects, it’s a good idea to wait a little while between document renders for this very reason.\nThe only real way to use R Projects is to keep them outside of OneDrive folders. That works well, but then you lose the backup features of OneDrive. It really depends on your company’s storage environment.\nIn lieu of R Projects, the best suggestion I have is to make a new folder for any project you’re working on and keep all scripts and outputs there. It’s not the same as a project but it will get the job done."
  },
  {
    "objectID": "posts/2022-09-20-onedrive-and-r/index.html#tip-3-always-keep-on-this-device",
    "href": "posts/2022-09-20-onedrive-and-r/index.html#tip-3-always-keep-on-this-device",
    "title": "Using R and RStudio with OneDrive/SharePoint",
    "section": "Tip 3: Always keep on this device",
    "text": "Tip 3: Always keep on this device\nThis option (via right-click in Windows Explorer) is a life-saver for reading data from shared locations. There’s just not a good way that I know of to access documents in the OneDrive/SharePoint cloud, so keeping them synced on your local PC is the only way. reaxl::read_xlsx attempting access a file seems to trigger Windows to download said file, but not always. And if a file is unchanged or unopened (at least as far as Windows knows) for a certain amount of time, that file may get removed locally to save storage space and then your scripts to read data will stop working."
  },
  {
    "objectID": "posts/2022-10-10-quarto-tables-workflow/index.html",
    "href": "posts/2022-10-10-quarto-tables-workflow/index.html",
    "title": "My Approach to Tables in Quarto Documents",
    "section": "",
    "text": "Whenever possible I try to use graphs and plots to back up my story about data analysis. That said, sometimes you’ve just got use some data tables. That could be summary statistics, regression/ANOVA tables, or maybe wedding the visual aids by creating sparklines or mini-plots within a table.\nIn 2022 there are no shortage of R libraries for making beautiful graphs and tables, but my most commonly used tools right now are kableExtra and flextable. gt looks promising but it’s still fairly new. I’ve tried using huxtable but for whatever reason it’s never quite clicked with me. DT is a great library but I try to avoid it for rmarkdown/quarto documents due to how large it blows up HTML files. It’s great when I need to include a full dataset in a report though, or in Shiny apps."
  },
  {
    "objectID": "posts/2022-10-10-quarto-tables-workflow/index.html#html-tables",
    "href": "posts/2022-10-10-quarto-tables-workflow/index.html#html-tables",
    "title": "My Approach to Tables in Quarto Documents",
    "section": "HTML Tables",
    "text": "HTML Tables\nI am unaware of any pdf-exclusive r table libraries, but that doesn’t mean they don’t exist. All of the previously mentioned tables can produce HTML tables, but I really only use:\n\nkableExtra\nflextable\nDT\n\nmostly in that order.\n\n\nCode\nlibrary(dplyr)\n\n\n\nkableExtra\nkableExtra is my go-to for HTML tables in my documents. I think the default settings look pretty good, and it’s easy to tweak them to get exactly what I want.\n\n\nCode\niris |> \n  group_by(Species) |> \n  summarise(across(.fns=list(mean=mean, sd=sd))) |> \n  kableExtra::kbl(\n    col.names = c(\"Species\", rep(c(\"Mean\", \"SD\"), 4)),\n    digits=1\n  ) |> \n  kableExtra::add_header_above(\n    c(\" \", \n      \"Sepal Length\"=2, \n      \"Sepal Width\"=2,\n      \"Petal Length\"=2,\n      \"Petal Width\"=2), \n    align = \"c\"\n  ) |> \n  kableExtra::kable_styling(\n    bootstrap_options = c(\"hover\", \"responsive\")\n  )\n\n\n\nIris Data - kableExtra\n \n\n\nSepal Length\nSepal Width\nPetal Length\nPetal Width\n\n  \n    Species \n    Mean \n    SD \n    Mean \n    SD \n    Mean \n    SD \n    Mean \n    SD \n  \n \n\n  \n    setosa \n    5.0 \n    0.4 \n    3.4 \n    0.4 \n    1.5 \n    0.2 \n    0.2 \n    0.1 \n  \n  \n    versicolor \n    5.9 \n    0.5 \n    2.8 \n    0.3 \n    4.3 \n    0.5 \n    1.3 \n    0.2 \n  \n  \n    virginica \n    6.6 \n    0.6 \n    3.0 \n    0.3 \n    5.6 \n    0.6 \n    2.0 \n    0.3 \n  \n\n\n\n\n\nIt’s pretty easy to get a nice-looking table, and I really like the “hover” and “responsive” bootstrap options in kableExtra::kable_styling(). Hover gives a quasi-interactive feel to the table, without having to load all the javascript required for sorting, filtering, etc. DT is better suited for that level of interactivity, but most tables probably don’t need that.\n\n\nflextable\nFlextable also makes pretty nice HTML tables, but they are more static than kableExtra’s.\n\n\nCode\niris |> \n  group_by(Species) |> \n  summarise(across(.fns=list(mean=mean, sd=sd))) |> \n  flextable::flextable() |> \n  flextable::set_header_labels(\n    Sepal.Length_mean = \"Mean\",\n    Sepal.Length_sd = \"SD\",\n    Sepal.Width_mean = \"Mean\",\n    Sepal.Width_sd = \"SD\",\n    Petal.Length_mean = \"Mean\",\n    Petal.Length_sd = \"SD\",\n    Petal.Width_mean = \"Mean\",\n    Petal.Width_sd = \"SD\"\n  ) |> \n  flextable::add_header_row(\n    values=c(\"\", \"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\"),\n    colwidths = c(1, 2, 2, 2, 2)\n  ) |> \n  flextable::colformat_double(j=2:9, digits=1) |> \n  flextable::align(i=1, j=2:9, align=\"center\", part=\"header\")\n\n\n\nIris data - flextableSepal LengthSepal WidthPetal LengthPetal WidthSpeciesMeanSDMeanSDMeanSDMeanSDsetosa5.00.43.40.41.50.20.20.1versicolor5.90.52.80.34.30.51.30.2virginica6.60.63.00.35.60.62.00.3\n\n\n\n\nSome differences\nOne of the big differences between the 2 packages is how you make changes. kableExtra, being based on knitr, leans towards to original R paradigm of a few functions with a lot of internal options, while flextable leans more towards to modern approach of several small functions that have a few options each. Neither approach is better than the other. With many small functions there are more commands that have to be remembered, but they are usually named in a way that easily explains what they do, and they are typically logicial to read and don’t require memorizing (and maintaining!) as much documentation. One drawback is having to nest all of those functions, or store output to variables over and over, but pipe functions (|> or %>%) have largely eliminate that problem. Now the only thing to watch for is long strings of spaghetti code, but that’s an issue regardless.\n\n\nDT\nDT is where I turn to in Shiny, or when I need to include a dataset as a table, but I try to avoid that.\n\n\nCode\niris |> \n  DT::datatable(\n    caption = \"Iris data- DT\",\n    filter=\"top\") |> \n  DT::formatRound(1:4, digits = 1)\n\n\n\n\n\n\n\nIt’s a pretty neat library, but I get overwhelmed with all of the options. Especially since many of the options are set by passing HTML/CSS and javascript code directly into the R code. Luckily there are some helpful guides from Rstudio and from the authors"
  },
  {
    "objectID": "posts/2022-10-10-quarto-tables-workflow/index.html#pdf-tables",
    "href": "posts/2022-10-10-quarto-tables-workflow/index.html#pdf-tables",
    "title": "My Approach to Tables in Quarto Documents",
    "section": "PDF Tables",
    "text": "PDF Tables\nkableExtra and flextable can also make really nice PDF tables as well. For small tables I lean towards flextable just because I think it looks “better” right away. For longer tables I lean towards kableExtra because of some of the row highlighting and spacing it does easily. Let’s see some examples\n\nkableExtra\n\n\nCode\niris |> \n  group_by(Species) |> \n  summarise(across(.fns=list(mean=mean, sd=sd))) |> \n  kableExtra::kbl(\n    booktabs = T,\n    col.names = c(\"Species\", rep(c(\"Mean\", \"SD\"), 4)),\n    digits=1\n  ) |> \n  kableExtra::add_header_above(\n    c(\" \", \n      \"Sepal Length\"=2, \n      \"Sepal Width\"=2,\n      \"Petal Length\"=2,\n      \"Petal Width\"=2), \n    align = \"c\"\n  ) |> \n  kableExtra::kable_classic_2() \n\n\n\n\n\nkableExtra PDF Table\n\n\nI ran the above code in a PDF format quarto document with mainfont: Cambria. I really don’t care for the default LaTeX font. Using the various styling functions in kableExtra speeds up the layout process. I tend not to use them for HTML tables though, just kable_styling().\nFor longer tables in a PDF document I make use of the latex_options flag in kable_styling():\n\n\nCode\niris |> \n  head(20) |> \n  kableExtra::kbl(\n    booktabs = T,\n    col.names = c(\"Sepal Length\", \"Sepal Width\", \n                  \"Petal Length\", \"Petal Width\",\n                  \"Species\"),\n    digits=1\n  ) |> \n  kableExtra::kable_classic_2() |> \n  kableExtra::kable_styling(latex_options = \"striped\")\n\n\n\n\n\nkableExtra Long PDF Table\n\n\nThe default of adding a little extra vertical padding every 5 lines, along with the striped option helps readability.\n\n\nFlextable\nFor this flextable example, I used the exact same code as the previous HTML example, but ran it in a PDF format quarto document. The library really lives up to it’s name!\n\n\n\nflextable PDF Table\n\n\nLonger flextables are where I run into issues though:\n\n\nCode\nevens <- function(x) subset(x, x %% 2 == 0)\nfives <- function(x) subset(x, x %% 5 == 0)\n\nirisSubset <- iris |> \n  head(20) \n\nirisSubset |> \n  flextable::flextable() |> \n  flextable::set_header_labels(\n    Sepal.Length=\"Sepal Length\", \n    Sepal.Width=\"Sepal Width\", \n    Petal.Length=\"Petal Length\", \n    Petal.Width=\"Petal Width\") |> \n  flextable::align(j=5, align=\"center\", part=\"all\") |> \n  flextable::bg(i=evens(c(1:length(irisSubset[,1]))), bg=\"#eeeeee\") |> \n  flextable::padding(i=fives(c(1:length(irisSubset[,1]))), padding.bottom = 20) |> \n  flextable::autofit()\n\n\n\n\n\nflextable Long PDF Table\n\n\nThe color and padding have to be manually defined which adds to code length and complexity. That’s not always a bad thing, but unfortunately not all of this code works in both HTML and PDF. flextable::padding() is the main issue. Theoretically you could add blank rows or something but that just further clutters up the code when you could just use kableExtra. In general, I find that simpler is better when it come to LaTeX and flextable."
  },
  {
    "objectID": "posts/2022-10-10-quarto-tables-workflow/index.html#microsoft-output",
    "href": "posts/2022-10-10-quarto-tables-workflow/index.html#microsoft-output",
    "title": "My Approach to Tables in Quarto Documents",
    "section": "Microsoft output",
    "text": "Microsoft output\nMost of the table libraries are focused on HTML, so outputting to Word usually involves using webshot to save a png and insert it into the document, or tossing out most of the formatting with a markdown table.\n\nkableExtra\nIf you use the prefer-html: true option in the YAML header, kableExtra can output simple markdown tables into a document:\n\n\n\nkableExtra Small Docx Table\n\n\n\n\n\nkableExtra Long Docx Table\n\n\nBut notice that they lose most of the formatting options that were applied. Also notice that while there is a caption, Word didn’t recognize it as the table caption so there is no numbering or document linking, and that’s even with using Quarto’s tbl-cap chunk option!\n\n\nflextable\nFlextable was designed to generate real MS Office tables though:\n\n\n\nflextable Small Docx Table\n\n\n\n\n\nflextable Long Docx Table\n\n\nThese tables look pretty good (if nothing else they’re drawn exactly as I told flextable). The padding option also works in Word documents. As before, the flextable output all uses the same code for HTML, PDF, and Docx report formats."
  },
  {
    "objectID": "posts/2022-10-10-quarto-tables-workflow/index.html#closing-thoughts",
    "href": "posts/2022-10-10-quarto-tables-workflow/index.html#closing-thoughts",
    "title": "My Approach to Tables in Quarto Documents",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nIn general, when I am making tables in an HTML document I’ll go with kableExtra because it very quickly gets me on the right track. However, there are many, many other options. When I am working with PDF documents I’ll use flextable for the simple stuff and kableExtra when I need better control over the LaTeX code being generated. When I am making Word documents, there’s really no better choice than flextable.\n\nQuarto/RMarkdown Tip\nI often create HTML and PDF/Docx reports for the same data. HTML is great for opening on a screen and interacting with the report, while PDF/Word can be more easily emailed, printed, etc. To facilitate my above mentioned table choices, I use the following code early in the document:\n\n\nCode\nis_html <- knitr::is_html_output()\n\n\nWhen the report is being compiled, the variable is_html will be TRUE for HTML documents and FALSE for all others. Then I can define functions like:\n\nSmall TableLong Table\n\n\n\n\nCode\nirisSumm <- iris |> \n  group_by(Species) |> \n  summarise(across(.fns=list(mean=mean, sd=sd))) \n\nif(is_html){\n  \n  irisSumm |> \n    kableExtra::kbl(\n      col.names = c(\"Species\", rep(c(\"Mean\", \"SD\"), 4)),\n      digits=1\n    ) |> \n    kableExtra::add_header_above(\n      c(\" \", \n        \"Sepal Length\"=2, \n        \"Sepal Width\"=2,\n        \"Petal Length\"=2,\n        \"Petal Width\"=2), \n      align = \"c\"\n    ) |> \n    kableExtra::kable_styling(\n      bootstrap_options = c(\"hover\", \"responsive\")\n    )\n  \n} else {\n  \n  irisSumm |> \n    flextable::flextable() |> \n    flextable::set_header_labels(\n      Sepal.Length_mean = \"Mean\",\n      Sepal.Length_sd = \"SD\",\n      Sepal.Width_mean = \"Mean\",\n      Sepal.Width_sd = \"SD\",\n      Petal.Length_mean = \"Mean\",\n      Petal.Length_sd = \"SD\",\n      Petal.Width_mean = \"Mean\",\n      Petal.Width_sd = \"SD\"\n    ) |> \n    flextable::add_header_row(\n      values=c(\"\", \"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\"),\n      colwidths = c(1, 2, 2, 2, 2)\n    ) |> \n    flextable::colformat_double(j=2:9, digits=1) |> \n    flextable::align(i=1, j=2:9, align=\"center\", part=\"header\")\n  \n}\n\n\n\n\n\n\nCode\nevens <- function(x) subset(x, x %% 2 == 0)\nfives <- function(x) subset(x, x %% 5 == 0)\n\nirisSubset <- iris |> \n  head(20)  \n\nif(is_html){\n  \n  irisSubset |> \n    kableExtra::kbl(\n      booktabs = T,\n      col.names = c(\"Sepal Length\", \"Sepal Width\", \n                    \"Petal Length\", \"Petal Width\",\n                    \"Species\"),\n      digits=1\n    ) |> \n    kableExtra::kable_styling(bootstrap_options = c(\"hover\", \"responsive\", \"condensed\"))\n  \n} else{\n\nirisSubset |> \n  flextable::flextable() |> \n  flextable::set_header_labels(\n    Sepal.Length=\"Sepal Length\", \n    Sepal.Width=\"Sepal Width\", \n    Petal.Length=\"Petal Length\", \n    Petal.Width=\"Petal Width\") |> \n  flextable::align(j=5, align=\"center\", part=\"all\") |> \n  flextable::bg(i=evens(c(1:length(irisSubset[,1]))), bg=\"#eeeeee\") |> \n  flextable::padding(i=fives(c(1:length(irisSubset[,1]))), padding.bottom = 20) |> \n  flextable::autofit()\n\n}"
  },
  {
    "objectID": "posts/2023-01-20-process-control-charts/index.html",
    "href": "posts/2023-01-20-process-control-charts/index.html",
    "title": "Process Control Charts",
    "section": "",
    "text": "Process Control Charts are great for analyzing any sort of process and identifying shifts in the process and outliers. They are a fundamental part of the Six Sigma program.\nThere are R libraries for making these charts:\n\nqcc\nqicharts2\nggQC\nAnd many more…\n\nBut I prefer to do them from scratch for more customization.\nThere are 3 main types of SPC Charts:\n\nX-Bar and S (Std. Dev, >10 samples per run/group)\nX-Bar and R (run range, <= 10 per run/group)\nX-Bar and M (change from run to run, when there is only 1 measure per group)\n\nA great resource for learning about Process Control Charts and other statistical methods is the NIST Handbook, which is referenced heavily for this post.\nNot only does it provide information and formulas, but data too. I will be using the LITHOGRA.DAT data for the charts in this post."
  },
  {
    "objectID": "posts/2023-01-20-process-control-charts/index.html#x-bar-and-s-charts",
    "href": "posts/2023-01-20-process-control-charts/index.html#x-bar-and-s-charts",
    "title": "Process Control Charts",
    "section": "X-bar and S Charts",
    "text": "X-bar and S Charts\n\nLithograph Data\nLet’s look at an example of creating a set of X-bar and S charts. We’ll use the Lithograph dataset from the NIST handbook. This data measures the width of lines etched in 5 locations on silicon wafers. There are 3 wafers per cassette, and 30 cassettes for a grand total of 450 measurements.\nThere are many different approaches we could take to how to group this data, but for now, lets consider the cassette as our “run” and examine the distribution of line widths for each cassette, regardless of the wafer number (1, 2, or 3) or the location of the line.\n\n\nReading the data\nWe’ll use readr::read_fwf() to read this data since it is in a fixed-width format. We’ll also drop the last column as it is not necessary (see the text in the file header for more details).\n\nlibrary(readr)\n\n# Load the link, or download the .DAT file and substitute in the next step.\nfileLitho <- read_file(\"https://www.itl.nist.gov/div898/handbook/datasets/LITHOGRA.DAT\")\n\n# read\ndataLitho <- read_fwf(\n  fileLitho, \n  col_positions = fwf_widths(\n    c(7, 7, 7, 12, 7, 12),\n    col_names=c(\"CASSETTE\", \"WAFER\", \"SITE\", \n                \"LINEWIDT\", \"RUNSEQ\",\"LINEWIDT_2\")\n    ),\n  skip=25\n) |> \n  subset(select=-c(LINEWIDT_2))\n\n\nhead(dataLitho)\n\n# A tibble: 6 × 5\n  CASSETTE WAFER  SITE LINEWIDT RUNSEQ\n     <dbl> <dbl> <dbl>    <dbl>  <dbl>\n1        1     1     1     3.20      1\n2        1     1     2     2.25      2\n3        1     1     3     2.07      3\n4        1     1     4     2.42      4\n5        1     1     5     2.39      5\n6        1     2     1     2.65      6\n\n\n\nsummary(dataLitho)\n\n    CASSETTE        WAFER        SITE      LINEWIDT          RUNSEQ     \n Min.   : 1.0   Min.   :1   Min.   :1   Min.   :0.7465   Min.   :  1.0  \n 1st Qu.: 8.0   1st Qu.:1   1st Qu.:2   1st Qu.:2.0505   1st Qu.:113.2  \n Median :15.5   Median :2   Median :3   Median :2.4533   Median :225.5  \n Mean   :15.5   Mean   :2   Mean   :3   Mean   :2.5323   Mean   :225.5  \n 3rd Qu.:23.0   3rd Qu.:3   3rd Qu.:4   3rd Qu.:2.9697   3rd Qu.:337.8  \n Max.   :30.0   Max.   :3   Max.   :5   Max.   :5.1687   Max.   :450.0  \n\n\n\n\nExplore the data\nThe first step of any data analysis, even when you have a clear goal in mind, should be to explore the data and get a feel for what is going on in there. Let’s load ggplot (we’ll need it later anyway) and poke around.\n\nlibrary(ggplot2)\n\nLet’s use box-and-whisker plots to check out the groups.\n\ndataLitho |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT, group=CASSETTE)) +\n  geom_boxplot()\n\n\n\n\nFigure 1: Linewidth by Cassette\n\n\n\n\n\nmodelCassette <- lm(LINEWIDT~CASSETTE, data=dataLitho)\nsummary(modelCassette)\n\n\nCall:\nlm(formula = LINEWIDT ~ CASSETTE, data = dataLitho)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.76979 -0.44244 -0.03813  0.40676  2.26966 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.03801    0.06160  33.086   <2e-16 ***\nCASSETTE     0.03189    0.00347   9.191   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6371 on 448 degrees of freedom\nMultiple R-squared:  0.1586,    Adjusted R-squared:  0.1568 \nF-statistic: 84.47 on 1 and 448 DF,  p-value: < 2.2e-16\n\n\nIt looks like there is some change in line width as the process goes on. This is exactly the sort of thing that SPC charts look for.\n\n\nInitial SPC Calculations\nThe first step of creating the SPC charts is to find the mean, standard deviation, and count of measurements in each group. This can be done in base R using aggregate() and transform(), or with dplyr functions. Both methods are shown below:\n\nbase Rdplyr\n\n\n\ndataLithoSumm <- aggregate(\n  # group LINEWIDT by CASSETTE\n  LINEWIDT~CASSETTE, \n  data=dataLitho,\n  # Calculate group mean, sd, and counts\n  FUN=function(x) c(mean=mean(x), \n                    sd=sd(x), \n                    count=length(x))\n)\n\n# We get a funky data.frame from aggregate with 1 normal column and 1 matrix column. \n# do.call will expand the matrix into separate columns. \n# It's a bit like nest and unnest in dplyr. \ndataLithoSumm <- do.call(data.frame, dataLithoSumm)\n\n\ndataLithoSumm <- transform(\n  dataLithoSumm,\n  \n  # Now we calculate the mean of means (x-bar-bar in the NIST handbook)\n  process.mean = mean(LINEWIDT.mean),\n\n  # and the mean of the standard deviations\n  process.sd = mean(LINEWIDT.sd)\n  )\n\nWith some nesting and use of R’s native pipe, this could all be run as one command, but it starts to look messy, and code legibility is important in itself.\n\n\n\nlibrary(dplyr)\n\ndataLithoSumm <- dataLitho |> \n  \n  # first set the group\n  group_by(CASSETTE) |> \n  \n  # Mean, SD, and Count for each run\n  summarise(LINEWIDT.mean = mean(LINEWIDT),\n            LINEWIDT.sd = sd(LINEWIDT),\n            LINEWIDT.count = n()) |> \n  \n  # Ungroup the data for the next step\n  ungroup() |> \n  \n  # Overall process Mean and SD\n  mutate(\n    process.mean = mean(LINEWIDT.mean),\n    process.sd = mean(LINEWIDT.sd)\n  ) \n\nOne of the great things about the tidyverse libraries is the ability to do many separate steps in a string of easy to understand commands.\n\n\n\nThe only difference in the output is that base R produces a data.frame and dplyr produces a tibble, but a tibble is really just a data.frame with some extensions\nNow that we have some summary data, let’s take a look at it:\n\nMeanStandard Deviation\n\n\n\ndataLithoSumm |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.mean)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(aes(yintercept = process.mean))\n\n\n\n\nFigure 2: Mean Linewidth by Cassette and Overall\n\n\n\n\n\n\n\ndataLithoSumm |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.sd)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(aes(yintercept = process.sd))\n\n\n\n\nFigure 3: Std. Dev. of Linewidth by Cassette\n\n\n\n\n\n\n\nIt looks like we’re off to a good start. We’ve now got the makings of a pair of X-bar and S charts, so now we need to add the next layer.\n\n\nControl Limits\nFor standard Shewhart control charts use a 3-sigma range, or +/- 3 standard deviations from the mean (the namesake of 6-Sigma). Technically, we do not know the true mean and standard deviation of the process, only the measured estimates. We account for this uncertainty with a weighting factor. There is a full explanation of this in the NIST Engineering Statistics Handbook.\nTo calculate the Control Limits, we’ll need the c\\(_{4}\\) parameter:\n\n# C4 Function\nc4 <- function(n) {\n  sqrt(2/(n-1)) * (factorial(n/2-1) / factorial((n-1)/2-1))\n}\n\nThen we use this parameter to calculate the Upper and Lower Control Limits:\n\nBase Rdplyr\n\n\n\ndataLithoSumm <- dataLithoSumm |> \n  transform(\n    \n    # X-bar chart UCL & LCL\n    xBar.UCL = process.mean + 3 * process.sd /\n      (c4(LINEWIDT.count)*sqrt(LINEWIDT.count)),\n    \n    xBar.LCL = process.mean - 3 * process.sd /\n      (c4(LINEWIDT.count)*sqrt(LINEWIDT.count)),\n    \n    # S chart UCL & LCL\n    s.UCL = process.sd + 3 * (process.sd/c4(LINEWIDT.count)) *\n      sqrt(1 - c4(LINEWIDT.count)^2),\n    \n    s.LCL = process.sd - 3 * (process.sd/c4(LINEWIDT.count)) *\n      sqrt(1 - c4(LINEWIDT.count)^2)\n    \n  )\n\n\n\n\ndataLithoSumm <- dataLithoSumm |> \n  mutate(\n    \n    # X-bar chart UCL & LCL\n    xBar.UCL = process.mean + 3 * process.sd /\n      (c4(LINEWIDT.count)*sqrt(LINEWIDT.count)),\n    \n    xBar.LCL = process.mean - 3 * process.sd /\n      (c4(LINEWIDT.count)*sqrt(LINEWIDT.count)),\n    \n    # S chart UCL & LCL\n    s.UCL = process.sd + 3 * (process.sd/c4(LINEWIDT.count)) *\n      sqrt(1 - c4(LINEWIDT.count)^2),\n    \n    s.LCL = process.sd - 3 * (process.sd/c4(LINEWIDT.count)) *\n      sqrt(1 - c4(LINEWIDT.count)^2)\n    \n  )\n\n\n\n\nThere’s a lot of repetitive code up there, some of that could be condensed if desired. Also, since every group in this dataset has the same number of measurements, the upper and lower control limits for the X-bar and S charts are the same for each group. Of course the mean of means and mean of standard deviations, process.mean and process.sd, are also the same for each group. These could be calculated as standalone variables if desired. Just remember that you can only do that if every row has the same sample size, otherwise you do have to calculate the limits for each row.\n\n\nPlotting the data\nNow that we have the data, lets make these charts.\n\ng1 <- dataLithoSumm |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.mean)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(aes(yintercept=process.mean)) +\n  geom_line(aes(y=xBar.UCL)) +\n  geom_line(aes(y=xBar.LCL)) +\n  labs(y=\"Avg. Linewidth\", x=\"Cassette\") +\n  theme_bw()\n\n\ng2 <- dataLithoSumm |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.sd)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(aes(yintercept=process.sd)) +\n  geom_line(aes(y=s.UCL)) +\n  geom_line(aes(y=s.LCL)) +\n  labs(y=\"StdDev. Linewidth\", x=\"Cassette\") +\n  theme_bw()\n\n\ngridExtra::grid.arrange(g1, g2)\n\n\n\n\nFigure 4: Lithograph X-Bar and S Charts\n\n\n\n\nGenerally the X-Bar and S charts are shown together with the x-axes aligned.\nIf desired, the Western Electric Company Rules can be applied to this chart. I like to mainly focus on shifting averages and extreme outliers:\n\ndplyr\n\ndataLithoSumm <- dataLithoSumm |> \n  mutate(\n    # Is the point beyond +/- 3 sigma?\n    xBar.beyond = ifelse((LINEWIDT.mean > xBar.UCL | LINEWIDT.mean < xBar.LCL), 1, 0),\n    # Assign each point -1, 0, or 1 if it is below, at, or above the mean\n    xBar.position = ifelse(LINEWIDT.mean > process.mean, 1, \n                       ifelse(LINEWIDT.mean < process.mean, -1, 0)),\n    # Then cumulatively sum these scores\n    xBar.csum = cumsum((xBar.position)),\n    # Use dplyr::lag to see if there are 7 or more consecutive points above or\n    # below the overall mean\n    xBar.lag = xBar.csum - lag(xBar.csum, 7, default = 0),\n    xBar.violatingRun = if_else(abs(xBar.lag)>=7, 1, 0),\n    xBar.color=if_else(xBar.beyond == 1, \"Beyond Limits\", \n                  if_else(xBar.violatingRun==1, \"Violating Run\", \"Normal\")),\n    \n    \n    \n    # Repeat for the standard deviation\n    s.beyond = ifelse((LINEWIDT.sd > s.UCL | LINEWIDT.sd < s.LCL), 1, 0),\n    s.position = ifelse(LINEWIDT.sd > process.sd, 1, \n                       ifelse(LINEWIDT.sd < process.sd, -1, 0)),\n    s.csum = cumsum((s.position)),\n\n    s.lag = s.csum - lag(s.csum, 7, default = 0),\n    s.violatingRun = if_else(abs(s.lag)>=7, 1, 0),\n    s.color=if_else(s.beyond == 1, \"Beyond Limits\", \n                  if_else(s.violatingRun==1, \"Violating Run\", \"Normal\"))\n  )\n\nI don’t know of a good way to emulate dplyr::lag() without some ugly code to shift row indexes around and then merge(df1, df2, all.x=TRUE).\n\ncolorsKey <- c(\"Beyond Limits\"=\"red\", \"Violating Run\"=\"orange\", \"Normal\"=\"black\")\n\ng1 <- dataLithoSumm |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.mean)) +\n  geom_point(aes(color=xBar.color)) +\n  geom_line() +\n  geom_hline(aes(yintercept=process.mean)) +\n  geom_line(aes(y=xBar.UCL)) +\n  geom_line(aes(y=xBar.LCL)) +\n  labs(y=\"Avg. Linewidth\", x=\"Cassette\") +\n  scale_color_manual(values = colorsKey) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\ng2 <- dataLithoSumm |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.sd)) +\n  geom_point(aes(color=s.color)) +\n  geom_line() +\n  geom_hline(aes(yintercept=process.sd)) +\n  geom_line(aes(y=s.UCL)) +\n  geom_line(aes(y=s.LCL)) +\n  labs(y=\"StdDev. Linewidth\", x=\"Cassette\") +\n  scale_color_manual(values = colorsKey) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\ngridExtra::grid.arrange(g1, g2)\n\n\n\n\nFigure 5: Lithograph X-Bar and S Charts With WECO Rules"
  },
  {
    "objectID": "posts/2023-01-20-process-control-charts/index.html#x-bar-and-r-charts",
    "href": "posts/2023-01-20-process-control-charts/index.html#x-bar-and-r-charts",
    "title": "Process Control Charts",
    "section": "X-bar and R Charts",
    "text": "X-bar and R Charts\nX-Bar and R charts are pretty similar to X-Bar and S charts, the just use Range instead of Standard Deviation. These charts are mostly used when groups are smaller than n=10. The Lithograph data has 15 measurements per cassette, but it will be fine to use as an example.\nWe’ll drop some of the data from the Lithograph dataset:\n\ndataLithoSubset <- dataLitho |> \n  filter(WAFER < 3)\n\nOr use subset() for base R.\n\nSPC Calculations\nThe calculations for the mean will be the same, but we’ll calculate Range instead of Standard Deviation.\n\nbase Rdplyr\n\n\n\ndataLithoRange <- aggregate(\n  # group LINEWIDT by CASSETTE\n  LINEWIDT~CASSETTE, \n  data=dataLithoSubset,\n  # Calculate group mean, sd, and counts\n  FUN=function(x) c(mean=mean(x), \n                    r=range(x)[2] - range(x)[1], \n                    count=length(x))\n)\n\n# We get a funky data.frame from aggregate with 1 normal column and 1 matrix column. \n# do.call will expand the matrix into separate columns. \n# It's a bit like nest and unnest in dplyr. \ndataLithoRange <- do.call(data.frame, dataLithoRange)\n\n\ndataLithoRange <- transform(\n  dataLithoRange,\n  \n  # Now we calculate the mean of means (x-bar-bar in the NIST handbook)\n  process.mean = mean(LINEWIDT.mean),\n\n  # and the mean of the standard deviations\n  process.range = mean(LINEWIDT.r)\n  )\n\n\n\n\ndataLithoRange <- dataLithoSubset |> \n  \n  # first set the group\n  group_by(CASSETTE) |> \n  \n  # Mean, SD, and Count for each run\n  summarise(LINEWIDT.mean = mean(LINEWIDT),\n            LINEWIDT.r = range(LINEWIDT)[2] - range(LINEWIDT)[1],\n            LINEWIDT.count = n()) |> \n  \n  # Ungroup the data for the next step\n  ungroup() |> \n  \n  # Overall process Mean and SD\n  mutate(\n    process.mean = mean(LINEWIDT.mean),\n    process.range = mean(LINEWIDT.r)\n  ) \n\n\n\n\n\n\nControl Limits\nFor X-Bar and R charts, A\\(_{2}\\) is generally used instead of c\\(_{4}\\) for the X-Bar unbiased estimator, and D\\(_{3}\\) and D\\(_{4}\\) are used for the Range:\n\ntablexBarR <- data.frame(\n  n=c(2:10),\n  a2 = c(1.880, 1.023, 0.729, 0.577, 0.483, 0.419, 0.373, 0.337, 0.308),\n  d3 = c(0, 0, 0, 0, 0, 0.076, 0.136, 0.184, 0.223),\n  d4 = c(3.267, 2.575, 2.282, 2.115, 2.004, 1.924, 1.864, 1.816, 1.777)\n)\n\nThen we use this parameter to calculate the Upper and Lower Control Limits:\n\nBase Rdplyr\n\n\n\ndataLithoRange <- \n  merge(dataLithoRange, tablexBarR, \n        by.x = \"LINEWIDT.count\", by.y = \"n\") |> \n  transform(\n    \n    # X-bar chart UCL & LCL\n    xBar.UCL = process.mean + a2 * process.range,\n    xBar.LCL = process.mean - a2 * process.range,\n    \n    # S chart UCL & LCL\n    r.UCL = process.range * d4,\n    r.LCL = process.range * d3\n    \n  )\n\n\n\n\ndataLithoRange <- dataLithoRange |> \n  left_join(tablexBarR, by=c(\"LINEWIDT.count\"=\"n\")) |> \n  mutate(\n    \n    # X-bar chart UCL & LCL\n    xBar.UCL = process.mean + a2 * process.range,\n    xBar.LCL = process.mean - a2 * process.range,\n    \n    # S chart UCL & LCL\n    r.UCL = process.range * d4,\n    r.LCL = process.range * d3\n    \n  )\n\n\n\n\n\n\nWECO Rules\n\ndataLithoRange <- dataLithoRange |> \nmutate(\n    \n    xBar.beyond = ifelse((LINEWIDT.mean > xBar.UCL | LINEWIDT.mean < xBar.LCL), 1, 0),\n    \n    xBar.position = ifelse(LINEWIDT.mean > process.mean, 1, \n                       ifelse(LINEWIDT.mean < process.mean, -1, 0)),\n    \n    xBar.csum = cumsum((xBar.position)),\n    \n    xBar.lag = xBar.csum - lag(xBar.csum, 7, default = 0),\n    xBar.violatingRun = if_else(abs(xBar.lag)>=7, 1, 0),\n    xBar.color=if_else(xBar.beyond == 1, \"Beyond Limits\", \n                  if_else(xBar.violatingRun==1, \"Violating Run\", \"Normal\")),\n    \n    \n    \n    \n    r.beyond = ifelse((LINEWIDT.r > r.UCL | LINEWIDT.r < r.LCL), 1, 0),\n    r.position = ifelse(LINEWIDT.r > process.range, 1, \n                       ifelse(LINEWIDT.r < process.range, -1, 0)),\n    r.csum = cumsum((r.position)),\n\n    r.lag = r.csum - lag(r.csum, 7, default = 0),\n    r.violatingRun = if_else(abs(r.lag)>=7, 1, 0),\n    r.color=if_else(r.beyond == 1, \"Beyond Limits\", \n                  if_else(r.violatingRun==1, \"Violating Run\", \"Normal\"))\n  )\n\n\n\nX-Bar and R Chart\n\ncolorsKey <- c(\"Beyond Limits\"=\"red\", \"Violating Run\"=\"orange\", \"Normal\"=\"black\")\n\ng1 <- dataLithoRange |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.mean)) +\n  geom_point(aes(color=xBar.color)) +\n  geom_line() +\n  geom_hline(aes(yintercept=process.mean)) +\n  geom_line(aes(y=xBar.UCL)) +\n  geom_line(aes(y=xBar.LCL)) +\n  labs(y=\"Avg. Linewidth\", x=\"Cassette\") +\n  scale_color_manual(values = colorsKey) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\ng2 <- dataLithoRange |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.r)) +\n  geom_point(aes(color=r.color)) +\n  geom_line() +\n  geom_hline(aes(yintercept=process.range)) +\n  geom_line(aes(y=r.UCL)) +\n  geom_line(aes(y=r.LCL)) +\n  labs(y=\"Range of Linewidths\", x=\"Cassette\") +\n  scale_color_manual(values = colorsKey) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\ngridExtra::grid.arrange(g1, g2)\n\n\n\n\nFigure 6: Lithograph X-Bar and R Charts With WECO Rules"
  },
  {
    "objectID": "posts/2023-01-20-process-control-charts/index.html#i-and-mr-charts",
    "href": "posts/2023-01-20-process-control-charts/index.html#i-and-mr-charts",
    "title": "Process Control Charts",
    "section": "I and MR Charts",
    "text": "I and MR Charts\nWhen the sample size is n=1, use the I and MR, or Individuals and Moving Range, chart. The Moving Range is the absolute difference between consecutive measurements.\n\nPrepare the data\n\ndataLithoMR <- dataLitho |>\n  # Filter down to one measurement per cassette\n  filter(WAFER == 1, SITE == 1) |> \n  mutate(\n    # Find the Moving Range\n    LINEWIDT.mr = abs(LINEWIDT - lag(LINEWIDT, 1)),\n    \n    process.mean = mean(LINEWIDT),\n    process.mr = mean(LINEWIDT.mr, na.rm=T),\n    \n    # 1.128 is the value for d2=1\n    xBar.UCL = process.mean + 3 * process.mr / 1.128,\n    xBar.LCL = process.mean - 3 * process.mr / 1.128,\n    \n    # 3.267 is D4 when n=2, 0 is D3 when n=2\n    mr.UCL = process.mr*3.267,\n    mr.LCL = process.mr*0,\n    \n    # Critical Limits for Individuals\n    xBar.beyond = if_else((LINEWIDT > xBar.UCL | LINEWIDT < xBar.LCL), 1, 0),\n    \n    xBar.position = ifelse(LINEWIDT > process.mean, 1, \n                       if_else(LINEWIDT < process.mean, -1, 0)),\n    \n    xBar.csum = cumsum((xBar.position)),\n    \n    xBar.lag = xBar.csum - lag(xBar.csum, 7, default = 0),\n    xBar.violatingRun = if_else(abs(xBar.lag)>=7, 1, 0),\n    xBar.color=if_else(xBar.beyond == 1, \"Beyond Limits\", \n                  if_else(xBar.violatingRun==1, \"Violating Run\", \"Normal\")),\n    \n    \n    \n    # Critical Limits for Moving Ranges\n    mr.beyond = if_else((LINEWIDT.mr > mr.UCL | LINEWIDT.mr < mr.LCL), 1, 0),\n    mr.beyond = if_else(is.na(mr.beyond), 0, mr.beyond),\n    \n    mr.position = if_else(LINEWIDT.mr > process.mr, 1, \n                       if_else(LINEWIDT.mr < process.mr, -1, 0)),\n    mr.position = if_else(is.na(mr.position), 0, mr.position),\n    mr.csum = cumsum(mr.position),\n\n    mr.lag = mr.csum - lag(mr.csum, 7, default = 0),\n    mr.violatingRun = if_else(abs(mr.lag)>=7, 1, 0),\n    mr.color=if_else(mr.beyond == 1, \"Beyond Limits\", \n                  if_else(mr.violatingRun==1, \"Violating Run\", \"Normal\"))\n    \n  ) \n\n\n\nI and MR Charts\n\ncolorsKey <- c(\"Beyond Limits\"=\"red\", \"Violating Run\"=\"orange\", \"Normal\"=\"black\")\n\ng1 <- dataLithoMR |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT)) +\n  geom_point(aes(color=xBar.color)) +\n  geom_line() +\n  geom_hline(aes(yintercept=process.mean)) +\n  geom_line(aes(y=xBar.UCL)) +\n  geom_line(aes(y=xBar.LCL)) +\n  labs(y=\"Linewidth\", x=\"Cassette\") +\n  scale_color_manual(values = colorsKey) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\ng2 <- dataLithoMR |> \n  ggplot(aes(x=CASSETTE, y=LINEWIDT.mr)) +\n  geom_point(aes(color=mr.color)) +\n  geom_line() +\n  geom_hline(aes(yintercept=process.mr)) +\n  geom_line(aes(y=mr.UCL)) +\n  geom_line(aes(y=mr.LCL)) +\n  labs(y=\"Moving Range of Linewidths\", x=\"Cassette\") +\n  scale_color_manual(values = colorsKey) +\n  theme_bw() +\n  theme(legend.position = \"none\")\n\n\ngridExtra::grid.arrange(g1, g2)\n\n\n\n\nFigure 7: Lithograph I and MR Charts With WECO Rules"
  }
]