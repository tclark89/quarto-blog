
## Lithograph Data

Let's look at an example of creating a set of X-bar and S charts.
  We'll use the 
  [Lithograph](https://www.itl.nist.gov/div898/handbook/datasets/LITHOGRA.DAT) 
  dataset from the NIST handbook. This data measures the width of lines
  etched in 5 locations on silicon wafers. 
  There are 3 wafers per cassette, and 30 cassettes for a grand 
  total of 450 measurements. 
  
There are many different approaches we could take to how to group this
  data, but for now, lets consider the cassette as our "run" and examine
  the distribution of line widths for each cassette, regardless of the
  wafer number (1, 2, or 3) or the location of the line. 
  
### Reading the data

We'll use `readr::read_fwf()` to read this data since it is in a 
  fixed-width format. We'll also drop the last column as it is not
  necessary (see the text in the file header for more details).
  
```{r}
#| label: load-litho-data
#| message: false
library(readr)

# Load the link, or download the .DAT file and substitute in the next step.
fileLitho <- read_file("https://www.itl.nist.gov/div898/handbook/datasets/LITHOGRA.DAT")

# read
dataLitho <- read_fwf(
  fileLitho, 
  col_positions = fwf_widths(
    c(7, 7, 7, 12, 7, 12),
    col_names=c("CASSETTE", "WAFER", "SITE", 
                "LINEWIDT", "RUNSEQ","LINEWIDT_2")
    ),
  skip=25
) |> 
  subset(select=-c(LINEWIDT_2))
```


```{r}
#| label: head-litho
head(dataLitho)
```

```{r}
#| label: summary-litho
summary(dataLitho)
```

### Explore the data

The first step of any data analysis, even when you have a clear goal
  in mind, should be to explore the data and get a feel for what is
  going on in there. Let's load `ggplot` (we'll need it later anyway)
  and poke around.
  
```{r}
#| label: load-ggplot
library(ggplot2)
```

  
```{r}
#| label: hitogram-litho
#| fig-cap: "Distribution of Linewidth"
dataLitho |> 
  ggplot(aes(x=LINEWIDT)) +
  geom_histogram(bins=10)

```

The data looks normal, so that's good. 
Let's use box-and-whisker plots to check out the groups.

```{r}
#| label: boxplot-litho-cassette
#| fig-cap: "Linewidth by Cassette"
dataLitho |> 
  ggplot(aes(x=CASSETTE, y=LINEWIDT, group=CASSETTE)) +
  geom_boxplot()
```
```{r}
#| label: model-litho-cassette
modelCassette <- lm(LINEWIDT~CASSETTE, data=dataLitho)
summary(modelCassette)
```

It looks like there is some change in line width as the process goes on. 
  This is exactly the sort of thing that SPC charts look for. 

```{r}
#| label: boxplot-litho-wafer
dataLitho |> 
  ggplot(aes(x=WAFER, y=LINEWIDT, group=WAFER)) +
  geom_boxplot()
```

```{r}
#| label: model-litho-wafer
modelWafer <- lm(LINEWIDT~WAFER, data=dataLitho)
summary(modelWafer)
```

Wafer number in the cassette doesn't look like it has any effect on the line
  width. That's good to know. 

```{r}
#| label: boxplot-litho-site
dataLitho |> 
  ggplot(aes(x=SITE, y=LINEWIDT, group=SITE)) +
  geom_boxplot()
```

  
```{r}
#| label: model-litho-site
modelSite <- lm(LINEWIDT~SITE, data=dataLitho)
summary(modelSite)

```
  

There appears to be a slight trend in line width based on the location of the 
  measurement on the wafer. For now we won't worry about this since it looks fairly
  consistent within site. In real-life you'd probably also want to look at all
  the various multi-factor groupings, but for brevity we'll just focus on the 
  measured line width by cassette number.
  
### Initial SPC Calculations

The first step of creating the SPC charts is to find the mean, standard deviation,
  and count of measurements in each group. This can be done in base R using
  `aggregate()` and `transform()`, or with `dplyr` functions. Both methods are
  shown below:
  
::: {.panel-tabset}

#### base R
```{r}
#| label: summ-litho-base

dataLithoSumm <- aggregate(
  # group LINEWIDT by CASSETTE
  LINEWIDT~CASSETTE, 
  data=dataLitho,
  # Calculate group mean, sd, and counts
  FUN=function(x) c(mean=mean(x), 
                    sd=sd(x), 
                    count=length(x))
)

# We get a funky data.frame from aggregate with 1 normal column and 1 matrix column. 
# do.call will expand the matrix into separate columns. 
# It's a bit like nest and unnest in dplyr. 
dataLithoSumm <- do.call(data.frame, dataLithoSumm)


dataLithoSumm <- transform(
  dataLithoSumm,
  
  # Now we calculate the mean of means (x-bar-bar in the NIST handbook)
  process.mean = mean(LINEWIDT.mean),

  # and the mean of the standard deviations
  process.sd = mean(LINEWIDT.sd)
  )


```

With some nesting and use of R's native pipe, this could all be run as one command,
  but it starts to look messy, and code legibility is important in itself. 
  I'd save that approach for use in an automated but well-documented function.

#### dplyr
```{r}
#| label: summ-litho-dplyr
#| message: false

library(dplyr)

dataLithoSumm <- dataLitho |> 
  
  # first set the group
  group_by(CASSETTE) |> 
  
  # Mean, SD, and Count for each run
  summarise(LINEWIDT.mean = mean(LINEWIDT),
            LINEWIDT.sd = sd(LINEWIDT),
            LINEWIDT.count = n()) |> 
  
  # Ungroup the data for the next step
  ungroup() |> 
  
  # Overall process Mean and SD
  mutate(
    process.mean = mean(LINEWIDT.mean),
    process.sd = mean(LINEWIDT.sd)
  ) 
```

:::

The only difference in the output is that base R produces a data.frame and dplyr
  produces a 
  [tibble](https://tibble.tidyverse.org/), 
  but a 
  [tibble is really just a data.frame with some extra polish.](https://r4ds.had.co.nz/tibbles.html)
  
Now that we have some summary data, let's take a look at it:

::: {.panel-tabset}

#### Mean

```{r}
#| label: litho-plot-mean
#| fig-cap: "Mean Linewidth by Cassette and Overall"

dataLithoSumm |> 
  ggplot(aes(x=CASSETTE, y=LINEWIDT.mean)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = process.mean))
```


#### Standard Deviation

```{r}
#| label: litho-plot-sd
#| fig-cap: "Std. Dev. of Linewidth by Cassette"

dataLithoSumm |> 
  ggplot(aes(x=CASSETTE, y=LINEWIDT.sd)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = process.sd))
```

:::

It looks like we're off to a good start. We've now got the makings
of a pair of X-bar and S charts, so now we need to add the next layer.

### Control Limits

For standard Shewhart control charts use a 3-sigma range, or +/-
  3 standard deviations from the mean (the namesake of 6-Sigma).
  Now technically, we do not know the true mean and standard 
  deviation of the process, only the measured estimates. We account
  for this uncertainty with a weighting factor. 
  There is a full explanation of this in the 
  [NIST Engineering Statistics Handbook](https://www.itl.nist.gov/div898/handbook/pmc/section3/pmc32.htm).
  
To calculate the Control Limits, we'll need the `c4` parameter:

```{r}
#| label: c4-function

# C4 Function
c4 <- function(n) {
  sqrt(2/(n-1)) * (factorial(n/2-1) / factorial((n-1)/2-1))
}
```

Then we use this parameter to calculate the Upper and Lower Control 
  Limits:

::: {.panel-tabset}
#### Base R
```{r}
#| label: base-r-control-limits

dataLithoSumm <- transform(
  dataLithoSumm,
  # X-bar UCL & LCL
  xBar.UCL = process.mean + 3 * process.sd /
    (c4(LINEWIDT.count)*sqrt(LINEWIDT.count)),
  xBar.LCL = process.mean - 3 * process.sd /
    (c4(LINEWIDT.count)*sqrt(LINEWIDT.count))
)

```

#### dplyr
```{r}
#| label: dplyr-control-limits

dataLithoSumm <- dataLithoSumm |> 
  mutate(
    xBar.UCL = process.mean + 3 * process.sd /
      (c4(LINEWIDT.count)*sqrt(LINEWIDT.count)),
  xBar.LCL = process.mean - 3 * process.sd /
    (c4(LINEWIDT.count)*sqrt(LINEWIDT.count))
  )

```


:::

